{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wN4vmiu1a_oM"
   },
   "source": [
    "#Fraud detection project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdd4tQPBbQt-"
   },
   "source": [
    "## Data importation and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdoqwtQqX3CS",
    "outputId": "22184239-6bad-466f-c26c-595960bc2691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Bnz3eEMX5xQK"
   },
   "outputs": [],
   "source": [
    "# Package to import \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QolVrBMEbgAf"
   },
   "source": [
    "Data's importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "uxUc6pvuZDXm",
    "outputId": "a9843435-8b02-4f75-cfca-b86ae40d76d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v21</th>\n",
       "      <th>v22</th>\n",
       "      <th>v23</th>\n",
       "      <th>v24</th>\n",
       "      <th>v25</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>amount</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time        v1        v2        v3        v4        v5        v6        v7  \\\n",
       "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5     2 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6     4  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7     7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8     7 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9     9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         v8        v9  ...       v21       v22       v23       v24       v25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "5  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427 -0.232794   \n",
       "6  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055  0.750137   \n",
       "7 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709 -0.415267   \n",
       "8  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592  0.373205   \n",
       "9  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794 -0.385050 -0.069733   \n",
       "\n",
       "        v26       v27       v28  amount  class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62  False  \n",
       "1  0.125895 -0.008983  0.014724    2.69  False  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66  False  \n",
       "3 -0.221929  0.062723  0.061458  123.50  False  \n",
       "4  0.502292  0.219422  0.215153   69.99  False  \n",
       "5  0.105915  0.253844  0.081080    3.67  False  \n",
       "6 -0.257237  0.034507  0.005168    4.99  False  \n",
       "7 -0.051634 -1.206921 -1.085339   40.80  False  \n",
       "8 -0.384157  0.011747  0.142404   93.20  False  \n",
       "9  0.094199  0.246219  0.083076    3.68  False  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    data = pd.read_csv(\"/content/drive/MyDrive/Data/creditcard.csv\", sep=',')\n",
    "except: \n",
    "    data = pd.read_csv(\"./Data/creditcard.csv\", sep=',')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "tFLpGq9XZhbj",
    "outputId": "b6c0306e-3411-499e-e53d-d7423a5c935c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-69b2fc1c-45bf-48a8-9221-5f9da3da7387\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v20</th>\n",
       "      <th>v21</th>\n",
       "      <th>v22</th>\n",
       "      <th>v23</th>\n",
       "      <th>v24</th>\n",
       "      <th>v25</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "      <td>284807.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.85958</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>88.34962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.14595</td>\n",
       "      <td>1.95870</td>\n",
       "      <td>1.65131</td>\n",
       "      <td>1.51626</td>\n",
       "      <td>1.41587</td>\n",
       "      <td>1.38025</td>\n",
       "      <td>1.33227</td>\n",
       "      <td>1.23709</td>\n",
       "      <td>1.19435</td>\n",
       "      <td>1.09863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77093</td>\n",
       "      <td>0.73452</td>\n",
       "      <td>0.72570</td>\n",
       "      <td>0.62446</td>\n",
       "      <td>0.60565</td>\n",
       "      <td>0.52128</td>\n",
       "      <td>0.48223</td>\n",
       "      <td>0.40363</td>\n",
       "      <td>0.33008</td>\n",
       "      <td>250.12011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>-56.40751</td>\n",
       "      <td>-72.71573</td>\n",
       "      <td>-48.32559</td>\n",
       "      <td>-5.68317</td>\n",
       "      <td>-113.74331</td>\n",
       "      <td>-26.16051</td>\n",
       "      <td>-43.55724</td>\n",
       "      <td>-73.21672</td>\n",
       "      <td>-13.43407</td>\n",
       "      <td>...</td>\n",
       "      <td>-54.49772</td>\n",
       "      <td>-34.83038</td>\n",
       "      <td>-10.93314</td>\n",
       "      <td>-44.80774</td>\n",
       "      <td>-2.83663</td>\n",
       "      <td>-10.29540</td>\n",
       "      <td>-2.60455</td>\n",
       "      <td>-22.56568</td>\n",
       "      <td>-15.43008</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.50000</td>\n",
       "      <td>-0.92037</td>\n",
       "      <td>-0.59855</td>\n",
       "      <td>-0.89036</td>\n",
       "      <td>-0.84864</td>\n",
       "      <td>-0.69160</td>\n",
       "      <td>-0.76830</td>\n",
       "      <td>-0.55408</td>\n",
       "      <td>-0.20863</td>\n",
       "      <td>-0.64310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.21172</td>\n",
       "      <td>-0.22839</td>\n",
       "      <td>-0.54235</td>\n",
       "      <td>-0.16185</td>\n",
       "      <td>-0.35459</td>\n",
       "      <td>-0.31715</td>\n",
       "      <td>-0.32698</td>\n",
       "      <td>-0.07084</td>\n",
       "      <td>-0.05296</td>\n",
       "      <td>5.60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.00000</td>\n",
       "      <td>0.01811</td>\n",
       "      <td>0.06549</td>\n",
       "      <td>0.17985</td>\n",
       "      <td>-0.01985</td>\n",
       "      <td>-0.05434</td>\n",
       "      <td>-0.27419</td>\n",
       "      <td>0.04010</td>\n",
       "      <td>0.02236</td>\n",
       "      <td>-0.05143</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.06248</td>\n",
       "      <td>-0.02945</td>\n",
       "      <td>0.00678</td>\n",
       "      <td>-0.01119</td>\n",
       "      <td>0.04098</td>\n",
       "      <td>0.01659</td>\n",
       "      <td>-0.05214</td>\n",
       "      <td>0.00134</td>\n",
       "      <td>0.01124</td>\n",
       "      <td>22.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.50000</td>\n",
       "      <td>1.31564</td>\n",
       "      <td>0.80372</td>\n",
       "      <td>1.02720</td>\n",
       "      <td>0.74334</td>\n",
       "      <td>0.61193</td>\n",
       "      <td>0.39856</td>\n",
       "      <td>0.57044</td>\n",
       "      <td>0.32735</td>\n",
       "      <td>0.59714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13304</td>\n",
       "      <td>0.18638</td>\n",
       "      <td>0.52855</td>\n",
       "      <td>0.14764</td>\n",
       "      <td>0.43953</td>\n",
       "      <td>0.35072</td>\n",
       "      <td>0.24095</td>\n",
       "      <td>0.09105</td>\n",
       "      <td>0.07828</td>\n",
       "      <td>77.16500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.00000</td>\n",
       "      <td>2.45493</td>\n",
       "      <td>22.05773</td>\n",
       "      <td>9.38256</td>\n",
       "      <td>16.87534</td>\n",
       "      <td>34.80167</td>\n",
       "      <td>73.30163</td>\n",
       "      <td>120.58949</td>\n",
       "      <td>20.00721</td>\n",
       "      <td>15.59499</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42090</td>\n",
       "      <td>27.20284</td>\n",
       "      <td>10.50309</td>\n",
       "      <td>22.52841</td>\n",
       "      <td>4.58455</td>\n",
       "      <td>7.51959</td>\n",
       "      <td>3.51735</td>\n",
       "      <td>31.61220</td>\n",
       "      <td>33.84781</td>\n",
       "      <td>25691.16000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69b2fc1c-45bf-48a8-9221-5f9da3da7387')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-69b2fc1c-45bf-48a8-9221-5f9da3da7387 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-69b2fc1c-45bf-48a8-9221-5f9da3da7387');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "               time            v1            v2            v3            v4  \\\n",
       "count  284807.00000  284807.00000  284807.00000  284807.00000  284807.00000   \n",
       "mean    94813.85958       0.00000       0.00000      -0.00000       0.00000   \n",
       "std     47488.14595       1.95870       1.65131       1.51626       1.41587   \n",
       "min         0.00000     -56.40751     -72.71573     -48.32559      -5.68317   \n",
       "25%     54201.50000      -0.92037      -0.59855      -0.89036      -0.84864   \n",
       "50%     84692.00000       0.01811       0.06549       0.17985      -0.01985   \n",
       "75%    139320.50000       1.31564       0.80372       1.02720       0.74334   \n",
       "max    172792.00000       2.45493      22.05773       9.38256      16.87534   \n",
       "\n",
       "                 v5            v6            v7            v8            v9  \\\n",
       "count  284807.00000  284807.00000  284807.00000  284807.00000  284807.00000   \n",
       "mean        0.00000       0.00000      -0.00000       0.00000      -0.00000   \n",
       "std         1.38025       1.33227       1.23709       1.19435       1.09863   \n",
       "min      -113.74331     -26.16051     -43.55724     -73.21672     -13.43407   \n",
       "25%        -0.69160      -0.76830      -0.55408      -0.20863      -0.64310   \n",
       "50%        -0.05434      -0.27419       0.04010       0.02236      -0.05143   \n",
       "75%         0.61193       0.39856       0.57044       0.32735       0.59714   \n",
       "max        34.80167      73.30163     120.58949      20.00721      15.59499   \n",
       "\n",
       "       ...           v20           v21           v22           v23  \\\n",
       "count  ...  284807.00000  284807.00000  284807.00000  284807.00000   \n",
       "mean   ...       0.00000       0.00000      -0.00000       0.00000   \n",
       "std    ...       0.77093       0.73452       0.72570       0.62446   \n",
       "min    ...     -54.49772     -34.83038     -10.93314     -44.80774   \n",
       "25%    ...      -0.21172      -0.22839      -0.54235      -0.16185   \n",
       "50%    ...      -0.06248      -0.02945       0.00678      -0.01119   \n",
       "75%    ...       0.13304       0.18638       0.52855       0.14764   \n",
       "max    ...      39.42090      27.20284      10.50309      22.52841   \n",
       "\n",
       "                v24           v25           v26           v27           v28  \\\n",
       "count  284807.00000  284807.00000  284807.00000  284807.00000  284807.00000   \n",
       "mean        0.00000       0.00000       0.00000      -0.00000      -0.00000   \n",
       "std         0.60565       0.52128       0.48223       0.40363       0.33008   \n",
       "min        -2.83663     -10.29540      -2.60455     -22.56568     -15.43008   \n",
       "25%        -0.35459      -0.31715      -0.32698      -0.07084      -0.05296   \n",
       "50%         0.04098       0.01659      -0.05214       0.00134       0.01124   \n",
       "75%         0.43953       0.35072       0.24095       0.09105       0.07828   \n",
       "max         4.58455       7.51959       3.51735      31.61220      33.84781   \n",
       "\n",
       "             amount  \n",
       "count  284807.00000  \n",
       "mean       88.34962  \n",
       "std       250.12011  \n",
       "min         0.00000  \n",
       "25%         5.60000  \n",
       "50%        22.00000  \n",
       "75%        77.16500  \n",
       "max     25691.16000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().apply(lambda s: s.apply('{0:.5f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UM8_S7bWZk5w",
    "outputId": "b95056f3-030c-478c-fb70-7cd31aaf6225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['time', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19', 'v20', 'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'amount', 'class']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Column names: {list(data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kqHfK9BMZuMJ",
    "outputId": "5ec85b36-81ed-41bc-fc53-c6d2da1e47a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data's shape: (284807, 31)\n",
      "------\n",
      "Data's dtypes:\n",
      "time        int64\n",
      "v1        float64\n",
      "v2        float64\n",
      "v3        float64\n",
      "v4        float64\n",
      "v5        float64\n",
      "v6        float64\n",
      "v7        float64\n",
      "v8        float64\n",
      "v9        float64\n",
      "v10       float64\n",
      "v11       float64\n",
      "v12       float64\n",
      "v13       float64\n",
      "v14       float64\n",
      "v15       float64\n",
      "v16       float64\n",
      "v17       float64\n",
      "v18       float64\n",
      "v19       float64\n",
      "v20       float64\n",
      "v21       float64\n",
      "v22       float64\n",
      "v23       float64\n",
      "v24       float64\n",
      "v25       float64\n",
      "v26       float64\n",
      "v27       float64\n",
      "v28       float64\n",
      "amount    float64\n",
      "class        bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data's shape: {data.shape}\\n------\")\n",
    "print(f\"Data's dtypes:\\n{data.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CC6ty61Qcwfr",
    "outputId": "262acabb-c08c-4210-c06d-7a66d795d32a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time      0\n",
       "v1        0\n",
       "v2        0\n",
       "v3        0\n",
       "v4        0\n",
       "v5        0\n",
       "v6        0\n",
       "v7        0\n",
       "v8        0\n",
       "v9        0\n",
       "v10       0\n",
       "v11       0\n",
       "v12       0\n",
       "v13       0\n",
       "v14       0\n",
       "v15       0\n",
       "v16       0\n",
       "v17       0\n",
       "v18       0\n",
       "v19       0\n",
       "v20       0\n",
       "v21       0\n",
       "v22       0\n",
       "v23       0\n",
       "v24       0\n",
       "v25       0\n",
       "v26       0\n",
       "v27       0\n",
       "v28       0\n",
       "amount    0\n",
       "class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgxTjR6pdXkF"
   },
   "source": [
    "There isn't any null observation or missing information in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OtHoTutbnNg"
   },
   "source": [
    "Let's take a look at the *time* variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-8cRq8Wa6gV",
    "outputId": "2e3b940e-6fde-4c44-996e-6cf3c652fe11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124592"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['time'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vQvx7Ked94Z"
   },
   "source": [
    "The time variable seems to not give any informations, so we chose to delete. We must delete it because it's a counter and it will be the most important variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "1XrqBw5md0Vm"
   },
   "outputs": [],
   "source": [
    "data.drop(columns='time', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KETP_U1_kHPn"
   },
   "source": [
    "Separating inputs (X) & output (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "2FTm-SDqeZOA",
    "outputId": "7eaa19fd-b561-4c09-dc9b-127d71d981ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1ce94165-4b4f-4566-8e85-28be37f55567\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v20</th>\n",
       "      <th>v21</th>\n",
       "      <th>v22</th>\n",
       "      <th>v23</th>\n",
       "      <th>v24</th>\n",
       "      <th>v25</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ce94165-4b4f-4566-8e85-28be37f55567')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1ce94165-4b4f-4566-8e85-28be37f55567 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1ce94165-4b4f-4566-8e85-28be37f55567');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         v1        v2        v3        v4        v5        v6        v7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         v8        v9       v10  ...       v20       v21       v22       v23  \\\n",
       "0  0.098698  0.363787  0.090794  ...  0.251412 -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974  ... -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643  ...  0.524980  0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952  ... -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074  ...  0.408542 -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        v24       v25       v26       v27       v28  amount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop(columns=\"class\")\n",
    "print(X.shape)\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-ztU3jajjlb",
    "outputId": "ff6c91d0-c000-4c49-8731-61c91c380206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: class, dtype: bool"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = data['class'].copy()\n",
    "print(Y.shape)\n",
    "Y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "LQjkpQDXi72G"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_normalized = StandardScaler().fit_transform(X)\n",
    "# Become a ndarray it's not a DataFrame anymore\n",
    "Y_prepross = LabelBinarizer().fit_transform(Y)\n",
    "\n",
    "Y_prepross = Y_prepross.reshape((Y_prepross.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "90iPwS9Gle-_"
   },
   "outputs": [],
   "source": [
    "x_train, x_test,y_train, y_test = train_test_split(X_normalized, Y_prepross,\\\n",
    "                                                   test_size=0.3, random_state=45,\\\n",
    "                                                   stratify=Y_prepross)\n",
    "# Stratify attribute permites use to have the same amount of fraud in our \n",
    "# subdatasets! It's a way to fight against bais du to imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IpatXaGemtyH",
    "outputId": "04626526-f36d-4ba1-cbff-6313a9e89884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:(199364, 29)\n",
      "x_test shape:(85443, 29)\n",
      "y_train shape: (199364,)\n",
      "y_test shape: (85443,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_train shape:{x_train.shape}\\nx_test shape:{x_test.shape}\\ny_train shape: {y_train.shape}\\ny_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPZ87TTi4EQz"
   },
   "source": [
    "Compute fraud rate in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXUutn073pBL",
    "outputId": "b5f25f95-65b7-47a6-85e4-33059b7c6638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud rate: 0.1727%\n"
     ]
    }
   ],
   "source": [
    "fraud_rate = (y_train.sum() + y_test.sum()) / (y_train.shape[0] + y_test.shape[0])\n",
    "# We can also use: \n",
    "# fraud_rate = data['class'].sum() / data.shape[0]\n",
    "print(f\"Fraud rate: {round(fraud_rate * 100, 4)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "BBI3kgJN6CE7"
   },
   "outputs": [],
   "source": [
    "def addlabels(x, y):\n",
    "    \"\"\"\n",
    "    Add the corresponding value for each segment of a barplot\n",
    "    \"\"\"\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i, y[i], round(y[i], 3), ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "_hAPUs-n5qZ7"
   },
   "outputs": [],
   "source": [
    "def plotDistribution(result_dict: dict, title: str, x_label: str, absolute=True) -> None:\n",
    "    \"\"\"\n",
    "    Display a barplot with absolute value or relative value\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    if absolute:\n",
    "        plt.bar(result_dict.keys(), result_dict.values(), color= ['red', 'blue', 'cyan', 'orange', 'green', 'yellow'])\n",
    "        plt.title(label=title)\n",
    "        plt.xlabel(x_label)\n",
    "        addlabels(result_dict.keys(), list(result_dict.values()))\n",
    "    else:\n",
    "        ratio = [(i/sum(result_dict.values()))*100 for i in list(result_dict.values())]\n",
    "\n",
    "        plt.bar(result_dict.keys(), ratio, color= ['red', 'blue', 'cyan', 'orange', 'green', 'yellow'])\n",
    "        plt.title(label=title)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel('en %')\n",
    "        addlabels(result_dict.keys(), ratio)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "4wg8KoTZ5uKt",
    "outputId": "3509cf59-edc4-4b03-ed44-c8c697838006"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fraud_rate_dict = {\"Fraud\": data[\"class\"].sum(), \"Not a Fraud\": (data['class'] == False).sum()}\n",
    "\n",
    "plotDistribution(fraud_rate_dict, \"Fraud Rate\", \"\", absolute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xc1mpjx_6VLk",
    "outputId": "66a07654-6e64-4b18-da5f-6c9d68fce352"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if we don't miss any observation\n",
    "((data['class'] == False).sum() + data['class'].sum()) == data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlyFn1w38s5o",
    "outputId": "f97b9ae7-dacf-4a7b-8490-452f0d70f1e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train fraud rate: 0.1725%\n",
      "\n",
      "y_test fraud rate: 0.1732%\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_train fraud rate: {round((y_train.sum()/y_train.shape[0])*100, 4)}%\\n\")\n",
    "print(f\"y_test fraud rate: {round((y_test.sum()/y_test.shape[0])*100, 4)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zelg7yeQ_gzD"
   },
   "outputs": [],
   "source": [
    "#TODO: dimensions reduction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztvp3CLF_8R6",
    "outputId": "6f6505cf-d1a1-46b8-ecbe-a27b29e52661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean recall score with k=2: 0.6308\n",
      "Mean recall score with k=3: 0.6336\n",
      "Mean recall score with k=4: 0.6192\n",
      "Mean recall score with k=5: 0.6247\n",
      "Mean recall score with k=6: 0.6246\n",
      "Mean recall score with k=7: 0.6163\n",
      "Mean recall score with k=8: 0.6192\n",
      "Mean recall score with k=9: 0.6194\n",
      "Mean recall score with k=10: 0.6247\n",
      "Mean recall score with k=11: 0.6198\n"
     ]
    }
   ],
   "source": [
    "from pandas.core.common import random_state\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "mean_recall_score = []\n",
    "for i in range(2,12):\n",
    "  stratified_k_fold = StratifiedKFold(n_splits=i, shuffle=True, random_state=55)\n",
    "\n",
    "  cv_lr_results = cross_validate(lr, x_train, y_train, cv=stratified_k_fold,\\\n",
    "                                scoring=\"recall\")\n",
    "  print(f\"Mean recall score with k={i}: {round(cv_lr_results['test_score'].sum() / len(cv_lr_results['test_score']), 4)}\")\n",
    "  mean_recall_score.append(round(cv_lr_results['test_score'].sum() / len(cv_lr_results['test_score']), 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XsswzyIAFGJ1",
    "outputId": "8a2a2506-b0fc-430d-d613-4836368b206b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "81 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 459, in _check_solver\n",
      "    solver\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1473, in fit\n",
      "    % self.l1_ratio\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.63358759 0.63358759        nan 0.61037376 0.57836257 0.61906941\n",
      " 0.61906941 0.58126112 0.59877956 0.57836257        nan        nan\n",
      " 0.59583016        nan 0.57836257        nan        nan        nan\n",
      "        nan        nan 0.63358759 0.63358759        nan 0.61037376\n",
      " 0.57836257 0.63358759 0.63358759 0.62779049 0.61037376 0.57836257\n",
      "        nan        nan 0.62779049        nan 0.57836257        nan\n",
      "        nan        nan        nan        nan 0.63358759 0.63358759\n",
      "        nan 0.61037376 0.57836257 0.63358759 0.63358759 0.63358759\n",
      " 0.61037376 0.57836257        nan        nan 0.63358759        nan\n",
      " 0.57836257        nan        nan        nan        nan        nan]\n",
      "  category=UserWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 0.1, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "Best recall score 0.6335875921688279\n"
     ]
    }
   ],
   "source": [
    "# Let's use GridSearchCV for testing different hyperparameters for LR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "stratified_k_fold = StratifiedKFold(n_splits=3, shuffle=True, random_state=55)\n",
    "\n",
    "lr_parameters = {\"penalty\": [\"none\", \"l2\", \"l1\", \"elasticnet\"],\n",
    "                 \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
    "                 \"C\": [0.1, 1, 10]}\n",
    "\n",
    "hpc = GridSearchCV(estimator=lr, \n",
    "                   param_grid=lr_parameters,\n",
    "                   scoring=\"recall\",\n",
    "                   verbose=1, \n",
    "                   cv=stratified_k_fold)\n",
    "hpc.fit(x_train, y_train)\n",
    "print(\"Best parameters:\", hpc.best_params_)\n",
    "print(\"Best recall score\", hpc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6PL7mC-cnNf",
    "outputId": "38578f36-265f-479d-d21b-60bdc48aa50a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best parameters: {'max_iter': 100}\n",
      "Best recall score 0.6335875921688279\n"
     ]
    }
   ],
   "source": [
    "stratified_k_fold = StratifiedKFold(n_splits=3, shuffle=True, random_state=55)\n",
    "\n",
    "lr_parameters = {\"max_iter\": [100, 1000, 10000, 100000]}\n",
    "\n",
    "hpc = GridSearchCV(estimator=lr, \n",
    "                   param_grid=lr_parameters,\n",
    "                   scoring=\"recall\",\n",
    "                   verbose=1, \n",
    "                   cv=stratified_k_fold)\n",
    "hpc.fit(x_train, y_train)\n",
    "print(\"Best parameters:\", hpc.best_params_)\n",
    "print(\"Best recall score\", hpc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjKuOiHa0nSO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
